# auto-evol-instruct
Automatic Instruction Evolution is an framework designed to automatically enhance the complexity of instructions used for LLM fine-tuning without human effort.

## Keywords
- Method: Prompt to enhance the complexity of instructions
- Trajectory: Every stages of instruction evolved by the method

## Key Features
- Automatic instruction complexity enhancement without human effort
- Support for various types of data
- Flexible configuration for method optimization process

## Introduction
This repository contains developed version of an automated algorithm for augmenting Korean language data through instruction evolution(https://github.com/joonavel/evol-instruct), based on the Automatic Instruction Evolving technique from the following paper:

Automatic Instruction Evolving for Large Language Models
(https://arxiv.org/abs/2406.00770)

Previous version of the implementation has limited number of evolving methods so that, it was not suitable for certain types of data. ex) Mathmatical problems, Code generation, etc.

This implementation has been developed to solve this problem by adding automatic evolving method optimization.

## Installation
```
pip install -r requirements.txt
```

## Usage

### Setting Environment Variables
Before running the project, you need to set the following environment variables in your .env file:
```
DEEPSEEK_API_KEY=your_deepseek_api_key
MY_HF_TOKEN=your_huggingface_api_key
```

### Required Parameters
- ```data_path <hf_dataset_name>```: Path to the hf dataset. There should be "instruction" and "response" fields in the dataset.
- ```train_size <int>```: Size of data to join method optimization.
- ```dev_size <int>```: Size of data to join method validation.
- ```seed <int>```: Random seed for reproducibility.
- ```batch_size <int>```: Batch size for whole process.
- ```max_steps <int>```: Maximum number of steps for method optimization.
- ```loop <int>```: Whole generation of method evolving.(l in the paper)
- ```candidate_size <int>```: Number of optimizatied methods.(m in the paper)
- ```test_run <int>```: Test run or not. if test_run is 1, only 10 instructions will be evolved.
- ```save_path <str>```: Path to save the evolution result.

### Optional Parameters
- ```evol_llm_config <str>```: Configuration for the evolving LLM.
- ```optim_llm_config <str>```: Configuration for the optimizing LLM.

### Model
Currently, Deepseek R1 is the only state-of-the-art LLM that is offered by API under the MIT License(Commercial use is allowed).
This framework is currently using DeepSeek R1 model for evolving and optimizing.
(https://deepseeklicense.github.io/)
If you want to use other models, you can change the model in the code.

### Example Usage
```bash
sh main.sh
```
Below is inside the script
```bash
python main.py\
    --data-path joonavel/seed_for_evolving\
    --train-size 20\
    --dev-size 50\
    --seed 42\
    --batch-size 10\
    --max-step 3\
    --loop 3\
    --candidate-size 5\
    --test-run 0\
    --save-path evolution_result.json
```
This command will...
1. Load the "joonavel/seed_for_evolving" dataset from the hf dataset.
2. Get 20 samples from the dataset for method optimization.
3. Get 50 samples from the dataset for method validation.
4. Method optimization process will be repeated 3 times(Method will be evolved 3 times).
5. The size of each trajectory of instruction evolving will be 3(loop).
6. The optimized method will be selected from 5 methods(candidate_size).
7. The evolution result will be saved to the "evolution_result.json" file.

## Components
This framework is composed of 5 main components.
- Evolver: Evolve the instruction by the method.
- Analyzer: Analyze the trajectory of instruction evolving and provide feedback.
- Optimizer: Optimize the method by the feedback.
- Validator: Validate the method by the dev dataset.
- Generator: Generate the instruction by the optimized method.

## Output
The script saves the results in JSON format to the specified output file.

Find a 2K dataset generated by this framework in the following link:
(https://huggingface.co/datasets/joonavel/ko-auto-evol-instruct)